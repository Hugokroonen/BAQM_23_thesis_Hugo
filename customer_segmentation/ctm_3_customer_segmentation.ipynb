{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0df62eea-e54d-4632-9b68-f0bbfa1166d1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# ReadMe\n",
    "\n",
    "This notebook requires: \n",
    "\n",
    "1. Estimated CTM-3 model in Python, then upload **counts_phi.csv** to Databricks (make sure to uncheck the box 'first row contains header')\n",
    "2. Run Data Pre-Processing notebook that processed the set of 'to-predict' customers, which uploads **y_segmentation.csv** and  **customers_segmentation.csv** to Databricks\n",
    "\n",
    "Then: \n",
    "\n",
    "3. Run this notebook to generate the M3 customer segmentation\n",
    "\n",
    "Note: Keep in mind that in different model fits, the order of motivations may vary. Hence, please revise the motivations_result.csv file in the python output, to identify the order of motivations and the corresponding labels (e.g. the healthy/conscious motivation may be motivation 1 in one fitted model, but become motivation 2 in the next fitted model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa31a5a-58b8-4b2c-a6d5-dadb8a7bae44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a43be47-59f9-439c-bd68-c7ae1ef5d87b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Prerequisites\n",
    "\n",
    "- upload counts_phi from python and y_segmentation from Data Pre Processing notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d493cf78-c5c4-4b56-82ff-519eb6c7448b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Load the motivations (counts_phi)\n",
    "- This file contains the 3 motivations, with corresponding product probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "761223f0-da4b-4e8b-a047-9112d8c9f089",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Upload latest counts_phi from fitted model \n",
    "df_phi_data = table('default.counts_phi')\n",
    "\n",
    "# Rename the columns using aliasing\n",
    "df_phi_data = df_phi_data.select(\n",
    "    F.col('_c0').alias('M0'),\n",
    "    F.col('_c1').alias('M1'),\n",
    "    F.col('_c2').alias('M2')\n",
    ")\n",
    "\n",
    "# Verify whether the probabilities add up to 1\n",
    "\n",
    "# Calculate the sum of the columns M0, M1, and M2\n",
    "column_sums = df_phi_data.agg(\n",
    "    F.sum('M0').alias('Sum_M0'),\n",
    "    F.sum('M1').alias('Sum_M1'),\n",
    "    F.sum('M2').alias('Sum_M2')\n",
    ").collect()[0]\n",
    "\n",
    "# Print the sums\n",
    "print(\"Sum of M0:\", column_sums['Sum_M0'])\n",
    "print(\"Sum of M1:\", column_sums['Sum_M1'])\n",
    "print(\"Sum of M2:\", column_sums['Sum_M2'])\n",
    "\n",
    "# If sums are 1, continue \n",
    "\n",
    "# Show the resulting counts_phi dataframe\n",
    "df_phi_data.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d9d32f2-a0ca-4291-8c0c-5816e699093b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. Loading the customers (y_segmentation and customers_segmentation)\n",
    "- Getting the purchase data from the customers we want to segment and getting their original customer IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e085d3c-d483-42e7-b9b6-f53691ade6c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load y_segmentation\n",
    "\n",
    "# Must be in the format customer_id, basket_id, product_id \n",
    "df_y_segmentation = table('default.y_segmentation') \n",
    "\n",
    "# Count how many unique products (lowest levels) are in y_segmentation\n",
    "unique_product_count = df_y_segmentation.select(\"product_id\").distinct().count()\n",
    "\n",
    "# Check: y_segmentation cannot contain more unique products than the amount of rows of counts_phi (otherwise the alignment of product ids failed)\n",
    "# Usually it is slightly less than the number of rows in counts_phi, as some lowest levels are not bought \n",
    "display(unique_product_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd252158-c10a-4ea4-9f14-0577ffffe793",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading the customers_segmentation dataset, to retrieve the original customer IDs\n",
    "df_customers_segmentation = table('default.customers_segmentation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdf4a0be-2010-4f64-9210-9acce67d8b5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_y_segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb7ea4dc-c925-43a3-b439-6fb8930c486a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Necessary pandas on spark imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355e943c-1954-46bc-a5b9-aa585de9169c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame.iteritems = pd.DataFrame.items\n",
    "import pyspark.pandas as ps \n",
    "from pyspark.pandas.config import option_context\n",
    " # Required option \n",
    "ps.set_option('compute.ops_on_diff_frames', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "400ec369-0398-400e-960a-f4c503d5ba60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Count amount of unique products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1171554-d316-41a4-9b7b-cfe36817a649",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get total number of unique products in transaction data  \n",
    "unique_products = df_phi_data.count() \n",
    "\n",
    "# Create an emtpy pandas-on-Spark 0 series with length equal to the amount of unique products   \n",
    "empty_customer_product_count = ps.Series(0, index=range(unique_products))\n",
    "\n",
    "print(\"Total number of unique products when fitting the model:\", unique_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a101a9b5-4641-491b-a26a-14bf4e89570e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff32c855-36da-4496-b784-9597ec44368e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Function to generate segmentation for each customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20f3aa6d-0fcb-4a09-9ab9-b5e2f8e5ae6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_customer_simple(df_y_segmentation, df_phi_data, i):\n",
    "\n",
    "  # Filter data on the specific customer \n",
    "  customer_data = df_y_segmentation.filter(F.col(\"customer_id\") == i)\n",
    "\n",
    "  # Group by product_id and count purchases\n",
    "  customer_product_counts = customer_data.groupBy(\"product_id\").count()\n",
    "\n",
    "  # Convert customer_product_counts to a Pandas DataFrame for further processing\n",
    "  customer_product_counts = customer_product_counts.toPandas()\n",
    "\n",
    "  # Create empty pandas-on-Spark series, with all unique product id's in it, to update with the product counts of the customer\n",
    "  customer_purchase_frequencies = ps.Series(0, index=range(unique_products))\n",
    "\n",
    "  # Iterate through product_counts_customer_product_counts and update customer_purchase_frequencies\n",
    "  for row in customer_product_counts.itertuples():\n",
    "      product_id = row.product_id\n",
    "      count = row.count\n",
    "      customer_purchase_frequencies.loc[product_id] = count\n",
    "\n",
    "  # Convert df_phi_data to pandas-on-Spark dataframe \n",
    "  phi_data = ps.DataFrame(df_phi_data)\n",
    "\n",
    "  # Calculate the dot product between product-purchase count vector and counts phi to obtain motivation probabilities for the customer \n",
    "  motivation_probabilities = customer_purchase_frequencies.dot(phi_data)\n",
    "\n",
    "  # Calculate sum of motivation probabilities for normalisation purposes \n",
    "  sum_motivation_probabilities = motivation_probabilities.sum()\n",
    "\n",
    "  # Apply normalisation to get motivation probabilities \n",
    "  if sum_motivation_probabilities > 0:\n",
    "    motivation_percentages = motivation_probabilities / sum_motivation_probabilities * 100\n",
    "  else:\n",
    "    motivation_percentages = ps.Series(0, index=range(len(motivation_probabilities)))\n",
    "\n",
    "  # Return probabilities of each motivation for the specific customer \n",
    "  return motivation_percentages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ddd3f60-78df-4d0b-a7aa-310db414d429",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Generate the segmentations for whole set of customers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3d3889a-5025-4458-95e1-171da0e9ef05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This needs to be optimised to become faster! \n",
    "\n",
    "# Initialize an empty list to store customer results\n",
    "customer_results = []\n",
    "\n",
    "# Necessary settings \n",
    "import pyspark.pandas as ps \n",
    "from pyspark.pandas.config import option_context\n",
    "# Set max rows \n",
    "with option_context(\n",
    "  'compute.max_rows', 2200, \"compute.ops_on_diff_frames\", True\n",
    "):\n",
    "  \n",
    "  # For each customer in the customer base, calculate the motivation probabilities \n",
    "  for i in range(50):\n",
    "\n",
    "      # Call the process_customer_simple function to calculate motivation percentages for the customer\n",
    "      motivation_percentages = process_customer_simple(df_y_segmentation, df_phi_data, i)\n",
    "\n",
    "      # Create a list for the current customer's results\n",
    "      customer_result = [i, motivation_percentages[0], motivation_percentages[1], motivation_percentages[2]]\n",
    "\n",
    "      # Append the customer result list to the list of results\n",
    "      customer_results.append(customer_result)\n",
    "\n",
    "      # Convert the list of lists to a DataFrame with appropriate column names\n",
    "      result_df = pd.DataFrame(customer_results, columns=['customer_id', 'M0', 'M1', 'M2'])\n",
    "\n",
    "\n",
    "# Convert resulting dataframe back to pyspark dataframe \n",
    "result_df = spark.createDataFrame(result_df)\n",
    "\n",
    "# Retrieve original customer IDs by joining customers_segmentation table \n",
    "result_df = result_df.join(df_customers_segmentation, \"customer_id\", \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8598612c-9b37-47d2-ac2f-1cd7b68d062d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Final segmentation table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8956b023-7c02-4758-90dd-66d714ac8d12",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the results for all customers \n",
    "result_df.select(\"CustomerID\", \"M0\", \"M1\", \"M2\").display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "CTM-3 Customer Segmentation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
